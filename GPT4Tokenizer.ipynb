{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPARfnkNQwX7w9B+iHU28xB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubbair/GPT4-Tokenizer/blob/main/GPT4Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use *Shakespear Poem*"
      ],
      "metadata": {
        "id": "OJ7uh0K9viY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A740Y0NXqfBU",
        "outputId": "a1e2ec84-8780-4593-ac0d-2e4f8d0b269d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 12:51:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-05 12:51:51 (20.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "M09Nm96qq5rH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cM6Yon6LVq7O"
      },
      "outputs": [],
      "source": [
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "      if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "        newids.append(idx)\n",
        "        i += 2\n",
        "      else:\n",
        "        newids.append(ids[i])\n",
        "        i += 1\n",
        "    return newids\n",
        "\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids,ids[1:]):\n",
        "        counts[pair] = counts.get(pair,0) + 1 # check if there are exist pair if not add 0 then +1\n",
        "    return counts\n",
        "\n",
        "class BasicTokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocabs:int = 0\n",
        "        self.merges:dict = {}\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "        assert vocab_size > 256\n",
        "        num_merges = vocab_size - 256\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        ids = list(tokens)\n",
        "        merges = {}\n",
        "        for i in range(num_merges):\n",
        "          stats = get_stats(ids)\n",
        "          pair = max(stats, key=stats.get)\n",
        "          idx = 256 + i\n",
        "          if verbose :\n",
        "             print(f\"merging {pair} into a new token {idx}\")\n",
        "\n",
        "          ids = merge(ids, pair, idx)\n",
        "          merges[pair] = idx\n",
        "\n",
        "        self.merges = merges\n",
        "        self.vocabs = vocab_size\n",
        "\n",
        "        print(\"tokens length:\", len(tokens))\n",
        "        print(\"ids length:\", len(ids))\n",
        "        print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        while True:\n",
        "            stats = get_stats(tokens)\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "            if pair not in self.merges:\n",
        "                break # nothing else can be merged\n",
        "            idx = self.merges[pair]\n",
        "            tokens = merge(tokens, pair, idx)\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, ids):\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        tokens = b\"\".join(vocab[idx] for idx in ids) # merge sequence of bytes\n",
        "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BasicTokenizer()\n",
        "text = \"aaabdaaabac\"\n",
        "tokenizer.train(text, 256 + 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLoB61Oa4FcK",
        "outputId": "a43854d1-26f4-4fb0-bb42-0a003120e589"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens length: 11\n",
            "ids length: 5\n",
            "compression ratio: 2.20X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "jbRmUvJa6Axi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert ids == [258, 100, 258, 97, 99]"
      ],
      "metadata": {
        "id": "JtqsfDXG4hrp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert tokenizer.decode(tokenizer.encode(text)) == text"
      ],
      "metadata": {
        "id": "uPIax_OX4lCC"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}