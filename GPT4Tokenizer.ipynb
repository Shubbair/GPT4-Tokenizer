{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubbair/GPT4-Tokenizer/blob/main/GPT4Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ7uh0K9viY_"
      },
      "source": [
        "# Use *Shakespear Poem*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A740Y0NXqfBU",
        "outputId": "515797cc-1411-4f85-ddc9-8df7a6e295f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-11 13:41:56--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-11 13:41:56 (22.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M09Nm96qq5rH"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFbgAbSCb-U4",
        "outputId": "80a77e89-960d-4867-983f-e9c32318775b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "Un8HoaKswNcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jryOt2IBBPKA"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import unicodedata\n",
        "import regex as re\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### set GPT4 patten to apply regex on it"
      ],
      "metadata": {
        "id": "BvWw5CgXwQ4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qjxo-XqMBDTw"
      },
      "outputs": [],
      "source": [
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXnvvEf7kX23"
      },
      "source": [
        "<h4>BPE (Byte Pair Encoding)</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kTptRoQok2kV"
      },
      "outputs": [],
      "source": [
        "# helper function used in get_gpt4_merges() to reconstruct the merges\n",
        "def bpe(mergeable_ranks : dict[bytes, int], token : list, max_rank : int) -> list:\n",
        "    parts = [bytes([b]) for b in token]\n",
        "    while True:\n",
        "        min_idx = None\n",
        "        min_rank = None\n",
        "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
        "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "            if rank is not None and (min_rank is None or rank < min_rank):\n",
        "                min_idx = i\n",
        "                min_rank = rank\n",
        "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
        "            break\n",
        "        assert min_idx is not None\n",
        "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
        "    return parts\n",
        "\n",
        "# get the merges from the gpt4\n",
        "def recover_merges(mergeable_ranks : dict[bytes, int]) -> list:\n",
        "    # the `merges` are already the byte sequences in their merged state.\n",
        "    # so we have to recover the original pairings. We can do this by doing\n",
        "    # a small BPE training run on all the tokens, in their order.\n",
        "    merges = {}\n",
        "    for token, rank in mergeable_ranks.items():\n",
        "        if len(token) == 1:\n",
        "            continue # skip raw bytes\n",
        "        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
        "        assert len(pair) == 2\n",
        "        # recover the integer ranks of the pair\n",
        "        ix0 = mergeable_ranks[pair[0]]\n",
        "        ix1 = mergeable_ranks[pair[1]]\n",
        "        merges[(ix0, ix1)] = rank\n",
        "\n",
        "    return merges\n",
        "\n",
        "# return counts of pairs\n",
        "def get_stats(ids : list[int]) -> list:\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]): # take two pairs of\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "# merge pair in ids with new idx\n",
        "def merge(ids : list[int], pair, idx) -> list:\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "def replace_control_characters(s: str) -> str:\n",
        "    # replace control chars\n",
        "    # Control characters -> http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
        "    chars = []\n",
        "    for ch in s:\n",
        "        if unicodedata.category(ch)[0] != \"C\":\n",
        "            chars.append(ch) # this character is ok\n",
        "        else:\n",
        "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
        "    return \"\".join(chars)\n",
        "\n",
        "\n",
        "def render_token(t: bytes) -> str:\n",
        "    # pretty print a token, escaping control characters\n",
        "    s = t.decode('utf-8', errors='replace')\n",
        "    s = replace_control_characters(s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "UakCv64-U_fk"
      },
      "outputs": [],
      "source": [
        "class GPT4Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        self.mergeable_ranks = enc._mergeable_ranks\n",
        "\n",
        "        print('Number of tokens:',len(self.mergeable_ranks))\n",
        "\n",
        "        self.merges = recover_merges(self.mergeable_ranks)\n",
        "\n",
        "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            vocab[idx] = vocab[p0] + vocab[p1]\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # shuffle values\n",
        "        self.byte_shuffle = {i: self.mergeable_ranks[bytes([i])] for i in range(256)}\n",
        "        # return index , value\n",
        "        self.inverse_byte_shuffle = {v: k for k, v in self.byte_shuffle.items()}\n",
        "\n",
        "    def encode_chunk(self,text_bytes : list[bytes],merges : dict[tuple,int]) -> list[int]:\n",
        "        # before we start processing bytes, we have to permute them\n",
        "        text_bytes = bytes(self.byte_shuffle[b] for b in text_bytes)\n",
        "        ids = list(text_bytes)\n",
        "        while len(ids) >= 2:\n",
        "            # find the pair with the lowest merge index\n",
        "            stats = get_stats(ids)\n",
        "            pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "            # subtle: if there are no more merges available, the key will\n",
        "            # result in an inf for every single pair, and the min will be\n",
        "            # just the first pair in the list, arbitrarily\n",
        "            # we can detect this terminating case by a membership check\n",
        "            if pair not in merges:\n",
        "                break # nothing else can be merged anymore\n",
        "            # otherwise let's merge the best pair (lowest merge index)\n",
        "            idx = merges[pair]\n",
        "            ids = merge(ids, pair, idx)\n",
        "        return ids\n",
        "\n",
        "    def encode(self,text: str) -> list[int]:\n",
        "        text_chunks = re.findall(self.pattern, text)\n",
        "        ids = []\n",
        "        for chunk in text_chunks:\n",
        "            chunk_bytes = chunk.encode(\"utf-8\")\n",
        "            chunk_ids = self.encode_chunk(chunk_bytes,self.merges)\n",
        "            ids.extend(chunk_ids)\n",
        "        return ids\n",
        "\n",
        "    def decode(self,ids:int)-> str:\n",
        "      # we have to un-permute the bytes before we decode\n",
        "      text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "      text_bytes = bytes(self.inverse_byte_shuffle[b] for b in text_bytes)\n",
        "      text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
        "      return text\n",
        "\n",
        "    def bpe_encode(self,mergeable_ranks: dict[bytes, int], input: bytes) -> list[int]:\n",
        "        parts = [bytes([b]) for b in input]\n",
        "        while True:\n",
        "            # Iterate over all pairs and find the pair we want to merge the most\n",
        "            min_idx = None\n",
        "            min_rank = None\n",
        "            for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
        "                rank = mergeable_ranks.get(pair[0] + pair[1])\n",
        "                if rank is not None and (min_rank is None or rank < min_rank):\n",
        "                    min_idx = i\n",
        "                    min_rank = rank\n",
        "\n",
        "            # If there were no pairs we could merge, we're done!\n",
        "            if min_rank is None:\n",
        "                break\n",
        "            assert min_idx is not None\n",
        "\n",
        "            # Otherwise, merge that pair and leave the rest unchanged. Then repeat.\n",
        "            parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2 :]\n",
        "        return parts\n",
        "\n",
        "    def visualise_tokens(self,text: str) -> None:\n",
        "      # tokenize the plain text\n",
        "      pat = re.compile(gpt4.pattern)\n",
        "      words = pat.findall(txt)\n",
        "      tokens = []\n",
        "      for word in words:\n",
        "          # Turn each word into tokens, using the byte pair encoding algorithm\n",
        "          word_bytes = word.encode(\"utf-8\")\n",
        "          word_tokens = self.bpe_encode(self.mergeable_ranks, word_bytes)\n",
        "          tokens.extend(word_tokens)\n",
        "\n",
        "      background = [f\"\\u001b[48;5;{i}m\" for i in [167, 179, 185, 77, 80, 68, 134]]\n",
        "      # If token boundaries do not occur at unicode character boundaries, it's unclear how best to\n",
        "      # visualise the token. Here, we'll just use the unicode replacement character to represent some\n",
        "      # fraction of a character.\n",
        "      unicode_token_values = [x.decode(\"utf-8\", errors=\"replace\") for x in tokens]\n",
        "      print(unicode_token_values)\n",
        "      running_length = 0\n",
        "      last_color = None\n",
        "      for token in unicode_token_values:\n",
        "          color = background[running_length % len(background)]\n",
        "          if color == last_color:\n",
        "              color = background[(running_length + 1) % len(background)]\n",
        "              assert color != last_color\n",
        "          last_color = color\n",
        "          running_length += len(token)\n",
        "          print(color + token, end=\"\")\n",
        "      print(\"\\u001b[0m\")\n",
        "\n",
        "    def save_vocab(self, vocab_file : str) -> None:\n",
        "      # build vocab being mindful of the byte shuffle\n",
        "      vocab = {idx: bytes([self.inverse_byte_shuffle[idx]]) for idx in range(256)}\n",
        "      for (p0, p1), idx in self.merges.items():\n",
        "          vocab[idx] = vocab[p0] + vocab[p1]\n",
        "      # now merge the shuffled bytes and write to file\n",
        "      inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
        "      with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "          for idx, token in vocab.items():\n",
        "              s = render_token(token)\n",
        "              if idx in inverted_merges:\n",
        "                  idx0, idx1 = inverted_merges[idx]\n",
        "                  s0 = render_token(vocab[idx0])\n",
        "                  s1 = render_token(vocab[idx1])\n",
        "                  f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
        "              else:\n",
        "                  f.write(f\"[{s}] {idx}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlGJjzIVVSw9",
        "outputId": "a63ecb02-6f23-4d0e-9c57-d0e9a99a0f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 100256\n"
          ]
        }
      ],
      "source": [
        "gpt4 = GPT4Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = 'iveodeost class not'"
      ],
      "metadata": {
        "id": "-INxMXi_oGjB"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HthIga-ajGIw",
        "outputId": "dc716ae3-f045-400e-ff2d-db8a5c9fde1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ive', 'ode', 'ost', ' class', ' not']\n",
            "\u001b[48;5;167mive\u001b[48;5;77mode\u001b[48;5;134most\u001b[48;5;185m class\u001b[48;5;179m not\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "gpt4.visualise_tokens(txt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4.save_vocab('gpt4.vocab')"
      ],
      "metadata": {
        "id": "awAa7Yzp2Prc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkhCAtJ2_3RE"
      },
      "source": [
        "#### test on karpathy test file : https://github.com/karpathy/minbpe/blob/master/tests/test_tokenizer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YLoB61Oa4FcK"
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tiktoken_ids = enc.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jbRmUvJa6Axi"
      },
      "outputs": [],
      "source": [
        "gpt4_tokenizer_ids = gpt4.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JtqsfDXG4hrp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cce072f-7fb9-458c-cf83-86b43c487cc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Same'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "'Same' if gpt4_tokenizer_ids == tiktoken_ids else 'Not Same'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Issue with GPT4 the tokenizer due to the process of merging tokens there are ambiguous tokens like :\n",
        "Token: ' attRot' </br>                    \n",
        "Token: '�' </br>  \n",
        "Token: 'EStreamFrame' </br>   \n",
        "Token: ' SolidGoldMagikarp' </br>  \n"
      ],
      "metadata": {
        "id": "h4rJ502Q2s9Y"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDT8f2GjZsLt97AGMIw7Hn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}